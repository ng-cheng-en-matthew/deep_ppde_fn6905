{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10359,
     "status": "ok",
     "timestamp": 1656558937935,
     "user": {
      "displayName": "nguwi jy",
      "userId": "17294038731682000497"
     },
     "user_tz": -480
    },
    "id": "cxxWRuDx5sIZ",
    "outputId": "28df6bb1-4eb8-4b91-ca80-5a24cafe38ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Colab Notebooks/deep_ppde\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: signatory in /usr/local/lib/python3.7/dist-packages (1.2.6.1.9.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# only run this cell if you start from Colab\n",
    "!git clone https://github.com/nguwijy/deep_ppde\n",
    "%cd deep_ppde\n",
    "!pip install signatory\n",
    "!pip install munch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h-sA2ZnR5RC3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-13 21:21:22.219139: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-13 21:21:22.307032: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-13 21:21:22.307060: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-13 21:21:22.776748: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-13 21:21:22.776802: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-13 21:21:22.776808: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-13 21:21:23.376144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 21:21:23.376342: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-13 21:21:23.376384: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-13 21:21:23.376418: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-13 21:21:23.376451: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-13 21:21:23.376484: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-13 21:21:23.376518: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-13 21:21:23.376550: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-13 21:21:23.376583: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-13 21:21:23.376592: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-03-13 21:21:23.376865: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "1 1 100 0 0.3018467996623872 0.7630312442779541\n",
      "1 1 100 1 0.30180652426678317 0.7275986671447754\n",
      "1 1 100 2 0.3021130893659228 0.7156596183776855\n",
      "1 1 100 3 0.30223769229312847 0.7205021381378174\n",
      "1 1 100 4 0.3018504288738592 0.7034204006195068\n",
      "1 1 100 5 0.30206750292914025 0.7138700485229492\n",
      "1 1 100 6 0.3020892781979723 0.7118310928344727\n",
      "1 1 100 7 0.3016283978468115 0.70809006690979\n",
      "1 1 100 8 0.30202348029892695 0.7067699432373047\n",
      "1 1 100 9 0.3021198461905333 0.7020955085754395\n",
      "1 0.1 10 0 0.30021585193709466 0.07957816123962402\n",
      "1 0.1 10 1 0.3001797378521864 0.0836644172668457\n",
      "1 0.1 10 2 0.3002258257363068 0.08324217796325684\n",
      "1 0.1 10 3 0.30030716430182164 0.07926630973815918\n",
      "1 0.1 10 4 0.3002015313477782 0.08235025405883789\n",
      "1 0.1 10 5 0.3003054374947938 0.07899260520935059\n",
      "1 0.1 10 6 0.30010980216756183 0.08183097839355469\n",
      "1 0.1 10 7 0.3001698235980442 0.08230185508728027\n",
      "1 0.1 10 8 0.3002322268313235 0.08689355850219727\n",
      "1 0.1 10 9 0.3002549135029642 0.08442187309265137\n",
      "10 0.1 10 0 0.30018872915774475 0.35251903533935547\n",
      "10 0.1 10 1 0.30020683085900135 0.31617283821105957\n",
      "10 0.1 10 2 0.30018054171063036 0.2969965934753418\n",
      "10 0.1 10 3 0.30019563638585584 0.30597496032714844\n",
      "10 0.1 10 4 0.3001840548697558 0.30377864837646484\n",
      "10 0.1 10 5 0.30020784312519005 0.28352928161621094\n",
      "10 0.1 10 6 0.30017839808811314 0.29843592643737793\n",
      "10 0.1 10 7 0.3001784278606481 0.2972249984741211\n",
      "10 0.1 10 8 0.3001744681134982 0.3099532127380371\n",
      "10 0.1 10 9 0.30018935438097893 0.30972838401794434\n",
      "2023-03-13 21:21:34.552592: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 4000000000 exceeds 10% of free system memory.\n",
      "2023-03-13 21:21:35.542804: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 4000000000 exceeds 10% of free system memory.\n",
      "2023-03-13 21:21:36.738238: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 4000000000 exceeds 10% of free system memory.\n",
      "2023-03-13 21:21:37.979500: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 4000000000 exceeds 10% of free system memory.\n",
      "2023-03-13 21:21:38.991166: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 4000000000 exceeds 10% of free system memory.\n",
      "100 0.1 10 0 0.30019763114569825 20.8365421295166\n",
      "100 0.1 10 1 0.30019337367319876 20.301864862442017\n",
      "100 0.1 10 2 0.3001959638837404 20.896596670150757\n",
      "100 0.1 10 3 0.3002027817942466 20.311041116714478\n",
      "100 0.1 10 4 0.30019694637739414 19.722934007644653\n",
      "100 0.1 10 5 0.30020480632662394 20.87922978401184\n",
      "100 0.1 10 6 0.3001988815921666 19.237061738967896\n",
      "100 0.1 10 7 0.3002003404463797 19.630053758621216\n",
      "100 0.1 10 8 0.30019152777603114 19.167717456817627\n",
      "100 0.1 10 9 0.3001952493429013 19.53206181526184\n",
      "1 0.1 10 0 0.30073916378411303 0.08130121231079102\n",
      "1 0.1 10 1 0.3006344835511884 0.08260846138000488\n",
      "1 0.1 10 2 0.30074577328687446 0.0818636417388916\n",
      "1 0.1 10 3 0.3009045799883591 0.07799386978149414\n",
      "1 0.1 10 4 0.30060905780633124 0.08175182342529297\n",
      "1 0.1 10 5 0.30083116091714407 0.07737946510314941\n",
      "1 0.1 10 6 0.3006165902576765 0.08013463020324707\n",
      "1 0.1 10 7 0.3006610108798391 0.07917213439941406\n",
      "1 0.1 10 8 0.30075500277271255 0.07887434959411621\n",
      "1 0.1 10 9 0.30074199217493436 0.08872008323669434\n",
      "10 0.1 10 0 0.3006645835840345 0.3404982089996338\n",
      "10 0.1 10 1 0.3006962615612335 0.29917240142822266\n",
      "10 0.1 10 2 0.30066928764455836 0.2813851833343506\n",
      "10 0.1 10 3 0.30070138243724687 0.3085203170776367\n",
      "10 0.1 10 4 0.3006871511655353 0.299640417098999\n",
      "10 0.1 10 5 0.30072368206593303 0.2993440628051758\n",
      "10 0.1 10 6 0.3006503225397879 0.30263447761535645\n",
      "10 0.1 10 7 0.300682715057826 0.31356048583984375\n",
      "10 0.1 10 8 0.30066565539529305 0.2997589111328125\n",
      "10 0.1 10 9 0.30067175876496016 0.28621721267700195\n",
      "100 0.1 10 0 0.30070037017105816 18.493394374847412\n",
      "100 0.1 10 1 0.30068825274932887 18.58978772163391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.1 10 2 0.30069283771971295 18.858948707580566\n",
      "100 0.1 10 3 0.3007097187470361 19.316373109817505\n",
      "100 0.1 10 4 0.30069742269009697 19.093255758285522\n",
      "100 0.1 10 5 0.30070450855341785 19.05765676498413\n",
      "100 0.1 10 6 0.3007042108280682 19.155450105667114\n",
      "100 0.1 10 7 0.3006952492950448 18.820653676986694\n",
      "100 0.1 10 8 0.30068080961558846 18.7091224193573\n",
      "100 0.1 10 9 0.30069894108938006 19.381174087524414\n",
      "Loss at 0 = 0.00011131690553156659\n",
      "Loss at 1 = 9.872838563751429e-05\n",
      "Loss at 2 = 0.00010787980863824487\n",
      "Loss at 3 = 8.137325494317338e-05\n",
      "Loss at 4 = 6.979153113206849e-05\n",
      "Loss at 5 = 7.19856980140321e-05\n",
      "Loss at 6 = 5.943270298303105e-05\n",
      "Loss at 7 = 6.047807983122766e-05\n",
      "Loss at 8 = 6.21397775830701e-05\n",
      "Loss at 9 = 5.647348007187247e-05\n",
      "1 0.1 10 0 0.30229205 40.48419785499573\n",
      "Loss at 0 = 9.601417696103454e-05\n",
      "Loss at 1 = 8.443003753200173e-05\n",
      "Loss at 2 = 8.110087219392881e-05\n",
      "Loss at 3 = 7.247055327752605e-05\n",
      "Loss at 4 = 7.190390897449106e-05\n",
      "Loss at 5 = 5.9643709391821176e-05\n",
      "Loss at 6 = 6.119359750300646e-05\n",
      "Loss at 7 = 4.6650289732497185e-05\n",
      "Loss at 8 = 5.168127972865477e-05\n",
      "Loss at 9 = 4.399732279125601e-05\n",
      "1 0.1 10 1 0.2986498 40.10462927818298\n",
      "Loss at 0 = 9.829933696892112e-05\n",
      "Loss at 1 = 8.652729593450204e-05\n",
      "Loss at 2 = 7.46374498703517e-05\n",
      "Loss at 3 = 6.838361150585115e-05\n",
      "Loss at 4 = 7.123140676412731e-05\n",
      "Loss at 5 = 5.648675505653955e-05\n",
      "Loss at 6 = 6.199444032972679e-05\n",
      "Loss at 7 = 6.195016612764448e-05\n",
      "Loss at 8 = 4.9845191824715585e-05\n",
      "Loss at 9 = 4.5894958020653576e-05\n",
      "1 0.1 10 2 0.29934868 40.41057372093201\n",
      "Loss at 0 = 0.0001186702138511464\n",
      "Loss at 1 = 0.00011330483539495617\n",
      "Loss at 2 = 9.959224553313106e-05\n",
      "Loss at 3 = 9.05050037545152e-05\n",
      "Loss at 4 = 8.888644515536726e-05\n",
      "Loss at 5 = 8.405906555708498e-05\n",
      "Loss at 6 = 7.528522837674245e-05\n",
      "Loss at 7 = 6.783277058275416e-05\n",
      "Loss at 8 = 6.714365736115724e-05\n",
      "Loss at 9 = 4.855859879171476e-05\n",
      "1 0.1 10 3 0.29924163 40.628660440444946\n",
      "Loss at 0 = 0.00010673228825908154\n",
      "Loss at 1 = 9.346274600829929e-05\n",
      "Loss at 2 = 9.326370491180569e-05\n",
      "Loss at 3 = 8.628646173747256e-05\n",
      "Loss at 4 = 7.681033457629383e-05\n",
      "Loss at 5 = 7.719912537140772e-05\n",
      "Loss at 6 = 6.479576404672116e-05\n",
      "Loss at 7 = 5.233562842477113e-05\n",
      "Loss at 8 = 5.96043755649589e-05\n",
      "Loss at 9 = 3.906808342435397e-05\n",
      "1 0.1 10 4 0.30262008 39.921489238739014\n",
      "Loss at 0 = 0.0001020073177642189\n",
      "Loss at 1 = 0.00011762580834329128\n",
      "Loss at 2 = 9.752854384714738e-05\n",
      "Loss at 3 = 9.508569200988859e-05\n",
      "Loss at 4 = 7.882218051236123e-05\n",
      "Loss at 5 = 9.002718434203416e-05\n",
      "Loss at 6 = 8.283142233267426e-05\n",
      "Loss at 7 = 5.809576759929769e-05\n",
      "Loss at 8 = 5.648942169500515e-05\n",
      "Loss at 9 = 4.602052649715915e-05\n",
      "1 0.1 10 5 0.30081782 39.953057289123535\n",
      "Loss at 0 = 0.00011218495637876913\n",
      "Loss at 1 = 8.48857598612085e-05\n",
      "Loss at 2 = 8.010327292140573e-05\n",
      "Loss at 3 = 7.890182314440608e-05\n",
      "Loss at 4 = 8.458435331704095e-05\n",
      "Loss at 5 = 6.894153193570673e-05\n",
      "Loss at 6 = 5.203331966185942e-05\n",
      "Loss at 7 = 6.10476708970964e-05\n",
      "Loss at 8 = 3.9202808693517e-05\n",
      "Loss at 9 = 3.738381201401353e-05\n",
      "1 0.1 10 6 0.30458343 40.48012685775757\n",
      "Loss at 0 = 0.00010216973896604031\n",
      "Loss at 1 = 9.98897448880598e-05\n",
      "Loss at 2 = 5.6226388551294804e-05\n",
      "Loss at 3 = 5.766649337601848e-05\n",
      "Loss at 4 = 5.1204806368332356e-05\n",
      "Loss at 5 = 4.03030717279762e-05\n",
      "Loss at 6 = 3.7458645238075405e-05\n",
      "Loss at 7 = 4.079344216734171e-05\n",
      "Loss at 8 = 4.024634836241603e-05\n",
      "Loss at 9 = 2.4403203497058712e-05\n",
      "1 0.1 10 7 0.3025613 40.478941917419434\n",
      "Loss at 0 = 0.00010955461038975045\n",
      "Loss at 1 = 9.18125151656568e-05\n",
      "Loss at 2 = 8.646939386380836e-05\n",
      "Loss at 3 = 7.64634387451224e-05\n",
      "Loss at 4 = 6.094150012359023e-05\n",
      "Loss at 5 = 6.832000508438796e-05\n",
      "Loss at 6 = 7.362006726907566e-05\n",
      "Loss at 7 = 5.5193966545630246e-05\n",
      "Loss at 8 = 5.287581734592095e-05\n",
      "Loss at 9 = 4.1951669118134305e-05\n",
      "1 0.1 10 8 0.30030328 41.12418532371521\n",
      "Loss at 0 = 0.00010799065785249695\n",
      "Loss at 1 = 9.943471377482638e-05\n",
      "Loss at 2 = 8.540194539818913e-05\n",
      "Loss at 3 = 7.030289270915091e-05\n",
      "Loss at 4 = 8.156190597219393e-05\n",
      "Loss at 5 = 6.593633588636294e-05\n",
      "Loss at 6 = 6.878783460706472e-05\n",
      "Loss at 7 = 6.408810440916568e-05\n",
      "Loss at 8 = 4.708264168584719e-05\n",
      "Loss at 9 = 4.357129000709392e-05\n",
      "1 0.1 10 9 0.3024096 40.22968602180481\n",
      "Loss at 0 = 1.2233746019774117e-05\n",
      "Loss at 1 = 8.068625902524218e-06\n",
      "Loss at 2 = 8.617047569714487e-06\n",
      "Loss at 3 = 8.054243153310381e-06\n",
      "Loss at 4 = 1.081254413293209e-05\n",
      "Loss at 5 = 7.080508112267125e-06\n",
      "Loss at 6 = 6.475710051745409e-06\n",
      "Loss at 7 = 5.163178684597369e-06\n",
      "Loss at 8 = 7.453902981069405e-06\n",
      "Loss at 9 = 1.8195986513092066e-06\n",
      "10 0.1 10 0 0.2998687 48.87727975845337\n",
      "Loss at 0 = 1.2848657206632197e-05\n",
      "Loss at 1 = 1.1094976798631251e-05\n",
      "Loss at 2 = 1.0916701285168529e-05\n",
      "Loss at 3 = 1.2554175555123948e-05\n",
      "Loss at 4 = 1.615352084627375e-05\n",
      "Loss at 5 = 6.557259894179879e-06\n",
      "Loss at 6 = 5.398071152740158e-06\n",
      "Loss at 7 = 1.1867203284054995e-05\n",
      "Loss at 8 = 0.0\n",
      "Loss at 9 = 0.0\n",
      "10 0.1 10 1 0.3017892 49.024168729782104\n",
      "Loss at 0 = 1.088632780010812e-05\n",
      "Loss at 1 = 1.0961404768750072e-05\n",
      "Loss at 2 = 9.034152753883973e-06\n",
      "Loss at 3 = 9.605166269466281e-06\n",
      "Loss at 4 = 8.059610991040245e-06\n",
      "Loss at 5 = 8.223846634791698e-06\n",
      "Loss at 6 = 6.5628801166894846e-06\n",
      "Loss at 7 = 7.357001322816359e-06\n",
      "Loss at 8 = 9.160427907772828e-06\n",
      "Loss at 9 = 2.2433987112435716e-08\n",
      "10 0.1 10 2 0.30204403 49.05212998390198\n",
      "Loss at 0 = 1.2041376976412721e-05\n",
      "Loss at 1 = 8.925717338570394e-06\n",
      "Loss at 2 = 8.428562068729661e-06\n",
      "Loss at 3 = 9.232048796548042e-06\n",
      "Loss at 4 = 8.911203622119501e-06\n",
      "Loss at 5 = 7.647176971659064e-06\n",
      "Loss at 6 = 7.690843631280586e-06\n",
      "Loss at 7 = 1.6053916624514386e-05\n",
      "Loss at 8 = 0.0\n",
      "Loss at 9 = 0.0\n",
      "10 0.1 10 3 0.30215573 48.21702241897583\n",
      "Loss at 0 = 9.354788744531106e-06\n",
      "Loss at 1 = 9.436243999516591e-06\n",
      "Loss at 2 = 1.050405262503773e-05\n",
      "Loss at 3 = 9.391380444867536e-06\n",
      "Loss at 4 = 9.06719287740998e-06\n",
      "Loss at 5 = 7.89509795140475e-06\n",
      "Loss at 6 = 9.784069334273227e-06\n",
      "Loss at 7 = 4.5959968701936305e-06\n",
      "Loss at 8 = 9.009647328639403e-06\n",
      "Loss at 9 = 8.881784197001252e-16\n",
      "10 0.1 10 4 0.3023997 49.154117822647095\n",
      "Loss at 0 = 1.0479168849997222e-05\n",
      "Loss at 1 = 1.1572557923500426e-05\n",
      "Loss at 2 = 1.1186933079443406e-05\n",
      "Loss at 3 = 9.028330168803222e-06\n",
      "Loss at 4 = 8.798759154160507e-06\n",
      "Loss at 5 = 7.736187399132177e-06\n",
      "Loss at 6 = 6.763475539628416e-06\n",
      "Loss at 7 = 5.832454917253926e-06\n",
      "Loss at 8 = 6.573834525624989e-06\n",
      "Loss at 9 = 3.033745088032447e-06\n",
      "10 0.1 10 5 0.30187014 49.681676149368286\n",
      "Loss at 0 = 1.0802769793372136e-05\n",
      "Loss at 1 = 1.0148306500923354e-05\n",
      "Loss at 2 = 1.1229176379856654e-05\n",
      "Loss at 3 = 9.127581506618299e-06\n",
      "Loss at 4 = 8.722732673049904e-06\n",
      "Loss at 5 = 8.551143764634617e-06\n",
      "Loss at 6 = 8.36144681670703e-06\n",
      "Loss at 7 = 6.312496680038748e-06\n",
      "Loss at 8 = 4.644233740691561e-06\n",
      "Loss at 9 = 2.6974753382091876e-06\n",
      "10 0.1 10 6 0.30147755 49.986698389053345\n",
      "Loss at 0 = 8.836868801154196e-06\n",
      "Loss at 1 = 9.356870577903464e-06\n",
      "Loss at 2 = 9.066918210010044e-06\n",
      "Loss at 3 = 9.484625479672104e-06\n",
      "Loss at 4 = 9.140669135376811e-06\n",
      "Loss at 5 = 8.48578565637581e-06\n",
      "Loss at 6 = 8.790533684077673e-06\n",
      "Loss at 7 = 6.609088814002462e-06\n",
      "Loss at 8 = 1.0081448635901324e-05\n",
      "Loss at 9 = 3.552713678800501e-15\n",
      "10 0.1 10 7 0.3023968 49.4937789440155\n",
      "Loss at 0 = 1.0781916898849886e-05\n",
      "Loss at 1 = 9.330527063866612e-06\n",
      "Loss at 2 = 9.82864003162831e-06\n",
      "Loss at 3 = 8.90105638973182e-06\n",
      "Loss at 4 = 9.363757271785289e-06\n",
      "Loss at 5 = 8.572194019507151e-06\n",
      "Loss at 6 = 2.2372019884642214e-05\n",
      "Loss at 7 = 5.035793947172351e-06\n",
      "Loss at 8 = 1.6415381554057973e-10\n",
      "Loss at 9 = 2.381692065789309e-10\n",
      "10 0.1 10 8 0.3001945 50.69604158401489\n",
      "Loss at 0 = 9.979091373679694e-06\n",
      "Loss at 1 = 8.937153324950486e-06\n",
      "Loss at 2 = 9.539894563204143e-06\n",
      "Loss at 3 = 9.883229722618125e-06\n",
      "Loss at 4 = 7.792849828547332e-06\n",
      "Loss at 5 = 7.825165994290728e-06\n",
      "Loss at 6 = 7.112497769412585e-06\n",
      "Loss at 7 = 6.622369710385101e-06\n",
      "Loss at 8 = 9.526397661829833e-06\n",
      "Loss at 9 = 0.0\n",
      "10 0.1 10 9 0.30154207 50.084110736846924\n",
      "Loss at 0 = 1.5013500842542271e-06\n",
      "Loss at 1 = 1.3464161838783184e-06\n",
      "Loss at 2 = 1.1388566463210736e-06\n",
      "Loss at 3 = 1.1694501154124737e-06\n",
      "Loss at 4 = 1.0455296433065087e-06\n",
      "Loss at 5 = 7.940777777548647e-07\n",
      "Loss at 6 = 6.531708436341432e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 7 = 4.6223664185163216e-07\n",
      "Loss at 8 = 2.3775615431986807e-07\n",
      "Loss at 9 = 1.022628204339071e-09\n",
      "100 0.1 10 0 0.30109295 231.27327370643616\n",
      "Loss at 0 = 1.603581040399149e-06\n",
      "Loss at 1 = 1.3935621154814726e-06\n",
      "Loss at 2 = 1.343243184237508e-06\n",
      "Loss at 3 = 1.3311354223333183e-06\n",
      "Loss at 4 = 1.0166448873860645e-06\n",
      "Loss at 5 = 9.462131629334181e-07\n",
      "Loss at 6 = 6.247623218769149e-07\n",
      "Loss at 7 = 2.872247364393843e-07\n",
      "Loss at 8 = 3.3433664725635026e-07\n",
      "Loss at 9 = 4.1651414761645356e-08\n",
      "100 0.1 10 1 0.30096385 242.143860578537\n",
      "Loss at 0 = 1.5758949984956416e-06\n",
      "Loss at 1 = 1.5436207831953652e-06\n",
      "Loss at 2 = 1.2591282256835257e-06\n",
      "Loss at 3 = 1.2141284742028802e-06\n",
      "Loss at 4 = 1.0988613894369337e-06\n",
      "Loss at 5 = 6.7177148821429e-07\n",
      "Loss at 6 = 5.634614126392989e-07\n",
      "Loss at 7 = 6.151111620056327e-07\n",
      "Loss at 8 = 1.479915994195835e-07\n",
      "Loss at 9 = 3.4254838965352974e-09\n",
      "100 0.1 10 2 0.30120787 238.97377824783325\n",
      "Loss at 0 = 1.483615733377519e-06\n",
      "Loss at 1 = 1.7138986549980473e-06\n",
      "Loss at 2 = 1.697984998827451e-06\n",
      "Loss at 3 = 9.860096952252206e-07\n",
      "Loss at 4 = 1.0072230907098856e-06\n",
      "Loss at 5 = 7.349206612161652e-07\n",
      "Loss at 6 = 6.662160672021855e-07\n",
      "Loss at 7 = 7.067267233651364e-07\n",
      "Loss at 8 = 2.1543561956605117e-07\n",
      "Loss at 9 = 3.6105898826299665e-10\n",
      "100 0.1 10 3 0.30099422 240.32670402526855\n",
      "Loss at 0 = 1.2283360319997882e-06\n",
      "Loss at 1 = 1.1333104339428246e-06\n",
      "Loss at 2 = 1.5424511730088852e-06\n",
      "Loss at 3 = 1.3406988728092983e-06\n",
      "Loss at 4 = 1.0453865115778171e-06\n",
      "Loss at 5 = 1.1873976291099098e-06\n",
      "Loss at 6 = 9.300017040914099e-07\n",
      "Loss at 7 = 1.1165654711930983e-07\n",
      "Loss at 8 = 2.18467057777616e-08\n",
      "Loss at 9 = 7.110482341809998e-12\n",
      "100 0.1 10 4 0.30125543 242.36562395095825\n",
      "Loss at 0 = 1.7833988295024028e-06\n",
      "Loss at 1 = 1.2397467799019068e-06\n",
      "Loss at 2 = 1.5302637166314526e-06\n",
      "Loss at 3 = 1.4475631360255647e-06\n",
      "Loss at 4 = 8.650354743622302e-07\n",
      "Loss at 5 = 6.780321655242005e-07\n",
      "Loss at 6 = 7.31957925381721e-07\n",
      "Loss at 7 = 5.837300705024973e-07\n",
      "Loss at 8 = 2.2893756579378532e-07\n",
      "Loss at 9 = 8.872178547392195e-09\n",
      "100 0.1 10 5 0.30124903 245.88015007972717\n",
      "Loss at 0 = 1.3126855264999904e-06\n",
      "Loss at 1 = 1.416861323377816e-06\n",
      "Loss at 2 = 1.2825142903238884e-06\n",
      "Loss at 3 = 1.3139845123077976e-06\n",
      "Loss at 4 = 8.07243850431405e-07\n",
      "Loss at 5 = 6.812399533373537e-07\n",
      "Loss at 6 = 6.623221224799636e-07\n",
      "Loss at 7 = 5.93824552197475e-07\n",
      "Loss at 8 = 7.081038688738772e-08\n",
      "Loss at 9 = 5.776906730758924e-13\n",
      "100 0.1 10 6 0.30100515 236.4254732131958\n",
      "Loss at 0 = 1.5001696738181636e-06\n",
      "Loss at 1 = 1.4774411738471827e-06\n",
      "Loss at 2 = 1.725312586131622e-06\n",
      "Loss at 3 = 1.109535105570103e-06\n",
      "Loss at 4 = 1.0792082321131602e-06\n",
      "Loss at 5 = 1.0261733223160263e-06\n",
      "Loss at 6 = 1.0606809155433439e-06\n",
      "Loss at 7 = 1.0174292519593564e-08\n",
      "Loss at 8 = 1.4544321302878416e-10\n",
      "Loss at 9 = 4.5174627927302424e-13\n",
      "100 0.1 10 7 0.30128995 239.92197012901306\n",
      "Loss at 0 = 1.565953994031588e-06\n",
      "Loss at 1 = 1.5902153336355696e-06\n",
      "Loss at 2 = 1.2870436876255553e-06\n",
      "Loss at 3 = 1.3406812513494515e-06\n",
      "Loss at 4 = 1.1505637758091325e-06\n",
      "Loss at 5 = 7.750161330477567e-07\n",
      "Loss at 6 = 7.949018936415087e-07\n",
      "Loss at 7 = 6.29085434411536e-07\n",
      "Loss at 8 = 1.7538701513331034e-07\n",
      "Loss at 9 = 3.3495364704094754e-08\n",
      "100 0.1 10 8 0.30117846 235.05817413330078\n",
      "Loss at 0 = 1.6296576177410316e-06\n",
      "Loss at 1 = 1.2699904345936375e-06\n",
      "Loss at 2 = 1.4342613212647848e-06\n",
      "Loss at 3 = 1.4874282214805135e-06\n",
      "Loss at 4 = 1.0059592341349344e-06\n",
      "Loss at 5 = 8.264074722319492e-07\n",
      "Loss at 6 = 6.401256769095198e-07\n",
      "Loss at 7 = 5.77221385356097e-07\n",
      "Loss at 8 = 1.5532106090176967e-07\n",
      "Loss at 9 = 3.102378576258502e-09\n",
      "100 0.1 10 9 0.30117297 239.23602628707886\n",
      "Loss at 0 = 107.76739501953125\n",
      "Loss at 1 = 6.911130905151367\n",
      "Loss at 2 = 51.88025665283203\n",
      "Loss at 3 = 1.815989375114441\n",
      "Loss at 4 = 0.09163254499435425\n",
      "Loss at 5 = 1.0510799884796143\n",
      "Loss at 6 = 0.9532606601715088\n",
      "Loss at 7 = 2.4431607723236084\n",
      "Loss at 8 = 1.1430244445800781\n",
      "Loss at 9 = 0.07141473889350891\n",
      "1 0.1 10 0 1.0007286 99.76253890991211\n",
      "Loss at 0 = 11.662493705749512\n",
      "Loss at 1 = 24.626907348632812\n",
      "Loss at 2 = 21.77147102355957\n",
      "Loss at 3 = 13.6556396484375\n",
      "Loss at 4 = 0.8297090530395508\n",
      "Loss at 5 = 0.9290712475776672\n",
      "Loss at 6 = 0.11705982685089111\n",
      "Loss at 7 = 0.0990813821554184\n",
      "Loss at 8 = 0.2359064519405365\n",
      "Loss at 9 = 0.27597320079803467\n",
      "1 0.1 10 1 1.0005966 101.26897144317627\n",
      "Loss at 0 = 37.16154861450195\n",
      "Loss at 1 = 18.630897521972656\n",
      "Loss at 2 = 0.41216301918029785\n",
      "Loss at 3 = 0.9914048314094543\n",
      "Loss at 4 = 0.38738468289375305\n",
      "Loss at 5 = 1.24338698387146\n",
      "Loss at 6 = 4.118186950683594\n",
      "Loss at 7 = 1.0801308155059814\n",
      "Loss at 8 = 0.30706730484962463\n",
      "Loss at 9 = 0.5316253900527954\n",
      "1 0.1 10 2 1.0007913 101.73661255836487\n",
      "Loss at 0 = 44.51021957397461\n",
      "Loss at 1 = 59.027626037597656\n",
      "Loss at 2 = 90.66502380371094\n",
      "Loss at 3 = 5.7377190589904785\n",
      "Loss at 4 = 2.654895305633545\n",
      "Loss at 5 = 0.7383246421813965\n",
      "Loss at 6 = 0.16056418418884277\n",
      "Loss at 7 = 0.1708928793668747\n",
      "Loss at 8 = 0.29562506079673767\n",
      "Loss at 9 = 0.4803403913974762\n",
      "1 0.1 10 3 1.0007277 101.3922917842865\n",
      "Loss at 0 = 112.73006439208984\n",
      "Loss at 1 = 7.772450923919678\n",
      "Loss at 2 = 4.029032230377197\n",
      "Loss at 3 = 1.6449711322784424\n",
      "Loss at 4 = 0.7852999567985535\n",
      "Loss at 5 = 1.6787866353988647\n",
      "Loss at 6 = 1.351862907409668\n",
      "Loss at 7 = 0.9583469033241272\n",
      "Loss at 8 = 0.05619325116276741\n",
      "Loss at 9 = 0.13296738266944885\n",
      "1 0.1 10 4 1.0010257 103.39150905609131\n",
      "Loss at 0 = 83.79551696777344\n",
      "Loss at 1 = 0.40371349453926086\n",
      "Loss at 2 = 2.711564540863037\n",
      "Loss at 3 = 1.1818386316299438\n",
      "Loss at 4 = 3.028498411178589\n",
      "Loss at 5 = 18.117151260375977\n",
      "Loss at 6 = 0.9025625586509705\n",
      "Loss at 7 = 0.7183972597122192\n",
      "Loss at 8 = 0.19019164144992828\n",
      "Loss at 9 = 0.662801206111908\n",
      "1 0.1 10 5 1.0007285 103.60860800743103\n",
      "Loss at 0 = 95.9613265991211\n",
      "Loss at 1 = 0.05353115499019623\n",
      "Loss at 2 = 0.19340795278549194\n",
      "Loss at 3 = 0.4766157269477844\n",
      "Loss at 4 = 0.37056970596313477\n",
      "Loss at 5 = 1.5632803440093994\n",
      "Loss at 6 = 0.09115321189165115\n",
      "Loss at 7 = 0.1539555788040161\n",
      "Loss at 8 = 0.19691424071788788\n",
      "Loss at 9 = 0.013871864415705204\n",
      "1 0.1 10 6 1.0008118 103.29603481292725\n",
      "Loss at 0 = 90.98062896728516\n",
      "Loss at 1 = 63.37383270263672\n",
      "Loss at 2 = 1.0995593070983887\n",
      "Loss at 3 = 0.9549294114112854\n",
      "Loss at 4 = 0.9444336295127869\n",
      "Loss at 5 = 0.702612042427063\n",
      "Loss at 6 = 0.5723361968994141\n",
      "Loss at 7 = 0.3492005467414856\n",
      "Loss at 8 = 0.2884790003299713\n",
      "Loss at 9 = 0.012996681965887547\n",
      "1 0.1 10 7 1.0007821 103.00213623046875\n",
      "Loss at 0 = 54.17155456542969\n",
      "Loss at 1 = 37.94588088989258\n",
      "Loss at 2 = 23.122379302978516\n",
      "Loss at 3 = 0.03850634768605232\n",
      "Loss at 4 = 0.14828616380691528\n",
      "Loss at 5 = 0.2392878234386444\n",
      "Loss at 6 = 0.1588907092809677\n",
      "Loss at 7 = 0.3704858720302582\n",
      "Loss at 8 = 0.4405955672264099\n",
      "Loss at 9 = 0.04049384221434593\n",
      "1 0.1 10 8 1.0016286 104.18543148040771\n",
      "Loss at 0 = 38.728694915771484\n",
      "Loss at 1 = 14.895143508911133\n",
      "Loss at 2 = 0.6966002583503723\n",
      "Loss at 3 = 0.2302006483078003\n",
      "Loss at 4 = 1.5231472253799438\n",
      "Loss at 5 = 0.24812473356723785\n",
      "Loss at 6 = 0.061800699681043625\n",
      "Loss at 7 = 0.07475975155830383\n",
      "Loss at 8 = 0.11087042093276978\n",
      "Loss at 9 = 0.10178162157535553\n",
      "1 0.1 10 9 1.0007777 104.40827488899231\n",
      "Loss at 0 = 0.4783089756965637\n",
      "Loss at 1 = 0.0739627331495285\n",
      "Loss at 2 = 0.1982751041650772\n",
      "Loss at 3 = 0.23790697753429413\n",
      "Loss at 4 = 0.08155632019042969\n",
      "Loss at 5 = 0.16177119314670563\n",
      "Loss at 6 = 0.07406594604253769\n",
      "Loss at 7 = 0.10164010524749756\n",
      "Loss at 8 = 0.04231390357017517\n",
      "Loss at 9 = 0.09714577347040176\n",
      "10 0.1 10 0 1.0009924 117.1848406791687\n",
      "Loss at 0 = 0.448026180267334\n",
      "Loss at 1 = 0.0456138551235199\n",
      "Loss at 2 = 0.11860770732164383\n",
      "Loss at 3 = 0.05660397931933403\n",
      "Loss at 4 = 0.10050041973590851\n",
      "Loss at 5 = 0.06970082968473434\n",
      "Loss at 6 = 0.09922552108764648\n",
      "Loss at 7 = 0.9329853653907776\n",
      "Loss at 8 = 0.2623274326324463\n",
      "Loss at 9 = 0.03707616776227951\n",
      "10 0.1 10 1 1.0009468 117.4726767539978\n",
      "Loss at 0 = 1.072312831878662\n",
      "Loss at 1 = 0.06682708114385605\n",
      "Loss at 2 = 0.19189895689487457\n",
      "Loss at 3 = 0.08208238333463669\n",
      "Loss at 4 = 0.18761418759822845\n",
      "Loss at 5 = 0.07851127535104752\n",
      "Loss at 6 = 0.049661651253700256\n",
      "Loss at 7 = 0.04892722889780998\n",
      "Loss at 8 = 0.08390535414218903\n",
      "Loss at 9 = 0.06522323936223984\n",
      "10 0.1 10 2 1.0008273 119.67493963241577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 0 = 1.4456099271774292\n",
      "Loss at 1 = 0.07615311443805695\n",
      "Loss at 2 = 0.1459287405014038\n",
      "Loss at 3 = 0.058492328971624374\n",
      "Loss at 4 = 0.2534240484237671\n",
      "Loss at 5 = 0.1054655984044075\n",
      "Loss at 6 = 0.043996427208185196\n",
      "Loss at 7 = 0.07921096682548523\n",
      "Loss at 8 = 0.06860671192407608\n",
      "Loss at 9 = 0.046214740723371506\n",
      "10 0.1 10 3 1.0010264 122.71383500099182\n",
      "Loss at 0 = 0.6588113307952881\n",
      "Loss at 1 = 0.06021861732006073\n",
      "Loss at 2 = 0.10885491222143173\n",
      "Loss at 3 = 0.05230778455734253\n",
      "Loss at 4 = 0.08118068426847458\n",
      "Loss at 5 = 0.29969409108161926\n",
      "Loss at 6 = 0.09428957849740982\n",
      "Loss at 7 = 0.09030560404062271\n",
      "Loss at 8 = 0.0806603729724884\n",
      "Loss at 9 = 0.014378589577972889\n",
      "10 0.1 10 4 1.0007254 128.22718143463135\n",
      "Loss at 0 = 0.49921876192092896\n",
      "Loss at 1 = 0.19642417132854462\n",
      "Loss at 2 = 0.07904507219791412\n",
      "Loss at 3 = 0.057804614305496216\n",
      "Loss at 4 = 0.10936308652162552\n",
      "Loss at 5 = 1.2802075147628784\n",
      "Loss at 6 = 0.10903982073068619\n",
      "Loss at 7 = 0.04558481276035309\n",
      "Loss at 8 = 0.05796975642442703\n",
      "Loss at 9 = 0.04278349131345749\n",
      "10 0.1 10 5 1.000694 121.76265525817871\n",
      "Loss at 0 = 0.9940323233604431\n",
      "Loss at 1 = 0.12497467547655106\n",
      "Loss at 2 = 0.3028554618358612\n",
      "Loss at 3 = 0.15354251861572266\n",
      "Loss at 4 = 0.21087726950645447\n",
      "Loss at 5 = 0.06451684236526489\n",
      "Loss at 6 = 0.23571637272834778\n",
      "Loss at 7 = 0.06698703020811081\n",
      "Loss at 8 = 0.04292669519782066\n",
      "Loss at 9 = 0.027896756306290627\n",
      "10 0.1 10 6 1.0011417 121.0189859867096\n",
      "Loss at 0 = 1.1434499025344849\n",
      "Loss at 1 = 0.8964871168136597\n",
      "Loss at 2 = 0.11427664756774902\n",
      "xiaolu_asian\n",
      "1 1 100 0 0.3029867861377202 5.170210123062134\n",
      "1 1 100 1 0.3028978015892616 5.127183437347412\n",
      "1 1 100 2 0.30244774724423473 5.15021014213562\n",
      "1 1 100 3 0.30114029648445284 5.166997194290161\n",
      "1 1 100 4 0.30247452232726435 5.177660942077637\n",
      "1 1 100 5 0.30269989726536206 5.183017015457153\n",
      "1 1 100 6 0.3028060379054002 5.154770851135254\n",
      "1 1 100 7 0.3013714255517242 5.178640127182007\n",
      "1 1 100 8 0.30131698519207534 5.15644383430481\n",
      "1 1 100 9 0.3015046321304484 5.24507737159729\n",
      "xiaolu_asian\n",
      "1 1 100 0 0.30109839242353603 5.188623666763306\n",
      "1 1 100 1 0.301587435762483 5.588021278381348\n",
      "1 1 100 2 0.30213250273057196 5.118504285812378\n",
      "1 1 100 3 0.3023411799104486 5.141640663146973\n",
      "1 1 100 4 0.3022203093642627 5.104861497879028\n",
      "1 1 100 5 0.3018290725309303 5.12491774559021\n",
      "1 1 100 6 0.3014645389155745 5.175622224807739\n",
      "1 1 100 7 0.3023265779731671 5.604346752166748\n",
      "1 1 100 8 0.30230181241445736 5.578356504440308\n",
      "1 1 100 9 0.30163310260522863 5.112566947937012\n",
      "10 1 100 0 0.30170383613904317 7.421752214431763\n",
      "10 1 100 1 0.3018499874123217 7.414735317230225\n",
      "10 1 100 2 0.30181783532849393 8.33826470375061\n",
      "10 1 100 3 0.3021268312233536 7.478567600250244\n",
      "10 1 100 4 0.3013901249229789 7.889344692230225\n",
      "10 1 100 5 0.30225839138361504 7.771129131317139\n",
      "10 1 100 6 0.30219727221019427 7.795233726501465\n",
      "10 1 100 7 0.3019050925513331 7.652647256851196\n",
      "10 1 100 8 0.3020103368623473 7.655757427215576\n",
      "10 1 100 9 0.30227468392745055 7.3932085037231445\n",
      "100 1 100 0 0.3019483303159104 206.35552048683167\n",
      "100 1 100 1 0.30200280409022917 178.99886965751648\n",
      "100 1 100 2 0.30203494170678974 188.93797898292542\n",
      "100 1 100 3 0.3019797170896769 187.03671216964722\n",
      "100 1 100 4 0.30202384202641835 192.86421871185303\n",
      "100 1 100 5 0.3020097785559147 189.65060949325562\n",
      "100 1 100 6 0.3020010586938204 191.4515552520752\n",
      "100 1 100 7 0.30194830121432736 195.7231273651123\n",
      "100 1 100 8 0.3019553871057779 200.5442578792572\n",
      "100 1 100 9 0.30199711873469803 199.8256015777588\n",
      "2023-03-13 23:33:20.518811: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-13 23:33:20.721408: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-13 23:33:20.721435: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-13 23:33:21.437142: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-13 23:33:21.437203: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-13 23:33:21.437211: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING:tensorflow:From /home/nguwijy/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From /home/nguwijy/.local/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "2023-03-13 23:33:23.000526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 23:33:23.000729: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-13 23:33:23.000794: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-13 23:33:23.000833: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-13 23:33:23.000870: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-13 23:33:23.000908: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-13 23:33:23.000945: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-13 23:33:23.000982: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-13 23:33:23.001019: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-13 23:33:23.001027: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-03-13 23:33:23.001465: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-13 23:33:23.017152: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 100 0 0.3035101 143.9406599998474\n",
      "1 1 100 1 0.3036617 142.39337944984436\n",
      "1 1 100 2 0.3021957 141.18911933898926\n",
      "1 1 100 3 0.3025796 139.3989987373352\n",
      "1 1 100 4 0.30457684 138.18094182014465\n",
      "1 1 100 5 0.30050778 137.40829300880432\n",
      "1 1 100 6 0.30560023 137.55845260620117\n",
      "1 1 100 7 0.29932764 137.30143427848816\n",
      "1 1 100 8 0.2971291 137.83208084106445\n",
      "1 1 100 9 0.30246603 137.67110419273376\n",
      "1 1 100 0 0.3045673966407776 18.09847593307495\n",
      "1 1 100 1 0.3031269609928131 19.678842782974243\n",
      "1 1 100 2 0.3032197952270508 17.81137990951538\n",
      "1 1 100 3 0.30340632796287537 17.908416748046875\n",
      "1 1 100 4 0.30341845750808716 18.25494623184204\n",
      "1 1 100 5 0.3038722574710846 19.768187522888184\n",
      "1 1 100 6 0.2959260642528534 18.24370527267456\n",
      "1 1 100 7 0.3013731837272644 18.170566082000732\n",
      "1 1 100 8 0.3030633330345154 20.197767734527588\n",
      "1 1 100 9 0.30410975217819214 18.66016936302185\n",
      "10 1 100 0 0.3034140169620514 462.9512286186218\n",
      "10 1 100 1 0.304951012134552 462.5595557689667\n",
      "10 1 100 2 0.3032780885696411 469.8440923690796\n",
      "10 1 100 3 0.3009430766105652 467.200838804245\n",
      "10 1 100 4 0.30162709951400757 464.1608638763428\n",
      "10 1 100 5 0.3045714497566223 466.24158930778503\n",
      "10 1 100 6 0.3007758855819702 466.76127910614014\n",
      "10 1 100 7 0.30315813422203064 470.7598385810852\n",
      "10 1 100 8 0.3019237518310547 468.8822612762451\n",
      "10 1 100 9 0.30078211426734924 470.96958446502686\n",
      "2023-03-14 01:17:35.005827: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 01:17:35.093853: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-14 01:17:35.093886: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-14 01:17:35.546576: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-14 01:17:35.546633: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-14 01:17:35.546640: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING:tensorflow:From /home/nguwijy/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "galerkin_asian\n",
      "2023-03-14 01:17:36.303430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-14 01:17:36.303642: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-14 01:17:36.303686: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-14 01:17:36.303721: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-14 01:17:36.303758: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-14 01:17:36.303794: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-14 01:17:36.303828: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-14 01:17:36.303862: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-14 01:17:36.303897: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-14 01:17:36.303906: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-03-14 01:17:36.304183: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 01:17:39.790584: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "session 0, dimension is 1, predicted solution is 0.3067305088043213, loss is 0.08775737881660461, and learning rate is 0.006676082499325275,                         elapsed is 6.714539527893066.\n",
      "\n",
      "session 1, dimension is 1, predicted solution is 0.3033687472343445, loss is 0.0969502255320549, and learning rate is 0.006676082499325275,                         elapsed is 6.828059196472168.\n",
      "\n",
      "session 2, dimension is 1, predicted solution is 0.30288559198379517, loss is 0.08416413515806198, and learning rate is 0.006676082499325275,                         elapsed is 6.937463045120239.\n",
      "\n",
      "session 3, dimension is 1, predicted solution is 0.298012912273407, loss is 0.08148017525672913, and learning rate is 0.006676082499325275,                         elapsed is 6.978144645690918.\n",
      "\n",
      "session 4, dimension is 1, predicted solution is 0.3056568205356598, loss is 0.09061674028635025, and learning rate is 0.006676082499325275,                         elapsed is 6.863908529281616.\n",
      "\n",
      "session 5, dimension is 1, predicted solution is 0.3036195635795593, loss is 0.08449290692806244, and learning rate is 0.006676082499325275,                         elapsed is 7.29952597618103.\n",
      "\n",
      "session 6, dimension is 1, predicted solution is 0.2973386347293854, loss is 0.08503041416406631, and learning rate is 0.006676082499325275,                         elapsed is 7.186383008956909.\n",
      "\n",
      "session 7, dimension is 1, predicted solution is 0.2988767623901367, loss is 0.08997157216072083, and learning rate is 0.006676082499325275,                         elapsed is 7.5022382736206055.\n",
      "\n",
      "session 8, dimension is 1, predicted solution is 0.30048519372940063, loss is 0.08543940633535385, and learning rate is 0.006676082499325275,                         elapsed is 7.457930326461792.\n",
      "\n",
      "session 9, dimension is 1, predicted solution is 0.3022656738758087, loss is 0.07114045321941376, and learning rate is 0.006676082499325275,                         elapsed is 7.343713998794556.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session 0, dimension is 10, predicted solution is 0.2877267599105835, loss is 0.05547107383608818, and learning rate is 0.006676082499325275,                         elapsed is 146.65726780891418.\n",
      "\n",
      "session 1, dimension is 10, predicted solution is 0.3111411929130554, loss is 0.035077471286058426, and learning rate is 0.006676082499325275,                         elapsed is 149.61487126350403.\n",
      "\n",
      "session 2, dimension is 10, predicted solution is 0.30831271409988403, loss is 0.022992124781012535, and learning rate is 0.006676082499325275,                         elapsed is 150.62655544281006.\n",
      "\n",
      "session 3, dimension is 10, predicted solution is 0.30986079573631287, loss is 0.03448038548231125, and learning rate is 0.006676082499325275,                         elapsed is 154.97890996932983.\n",
      "\n",
      "session 4, dimension is 10, predicted solution is 0.28448057174682617, loss is 0.08064427226781845, and learning rate is 0.006676082499325275,                         elapsed is 158.0443992614746.\n",
      "\n",
      "session 5, dimension is 10, predicted solution is 0.31012964248657227, loss is 0.030346808955073357, and learning rate is 0.006676082499325275,                         elapsed is 158.9261999130249.\n",
      "\n",
      "session 6, dimension is 10, predicted solution is 0.2986595630645752, loss is 0.010349382646381855, and learning rate is 0.006676082499325275,                         elapsed is 161.31109309196472.\n",
      "\n",
      "2023-03-14 02:05:59.587887: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 02:05:59.804684: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-14 02:05:59.804709: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-14 02:06:00.553441: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-14 02:06:00.553502: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-14 02:06:00.553509: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING:tensorflow:From /home/nguwijy/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2023-03-14 02:06:01.709149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-14 02:06:01.709351: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-14 02:06:01.709408: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-14 02:06:01.709448: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-14 02:06:01.709571: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-14 02:06:01.709686: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-14 02:06:01.709801: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-14 02:06:01.709921: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-14 02:06:01.709964: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-14 02:06:01.709973: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-03-14 02:06:01.710651: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Begin to solve...\n",
      "WARNING:tensorflow:From /home/nguwijy/.local/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "2023-03-14 02:06:33.710700: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "step:     0,  loss: 6.6656e-02,  Y0: 4.5047e-01,  runtime:   52\n",
      "step:   100,  loss: 1.0635e-01,  Y0: 4.0373e-01,  runtime:   79\n",
      "step:   200,  loss: 8.5117e-02,  Y0: 3.6757e-01,  runtime:   83\n",
      "step:   300,  loss: 6.9396e-02,  Y0: 3.4156e-01,  runtime:   87\n",
      "step:   400,  loss: 6.2753e-02,  Y0: 3.2389e-01,  runtime:   91\n",
      "step:   500,  loss: 5.0818e-02,  Y0: 3.1283e-01,  runtime:   95\n",
      "step:   600,  loss: 4.3843e-02,  Y0: 3.0755e-01,  runtime:   99\n",
      "step:   700,  loss: 3.6833e-02,  Y0: 3.0373e-01,  runtime:  103\n",
      "step:   800,  loss: 3.2188e-02,  Y0: 3.0353e-01,  runtime:  107\n",
      "step:   900,  loss: 2.5166e-02,  Y0: 3.0472e-01,  runtime:  111\n",
      "step:  1000,  loss: 2.0375e-02,  Y0: 3.0524e-01,  runtime:  114\n",
      "step:  1100,  loss: 1.6439e-02,  Y0: 3.0347e-01,  runtime:  118\n",
      "step:  1200,  loss: 1.2744e-02,  Y0: 3.0392e-01,  runtime:  122\n",
      "step:  1300,  loss: 1.0467e-02,  Y0: 3.0386e-01,  runtime:  126\n",
      "step:  1400,  loss: 8.2641e-03,  Y0: 3.0306e-01,  runtime:  130\n",
      "step:  1500,  loss: 6.7719e-03,  Y0: 3.0386e-01,  runtime:  134\n",
      "step:  1600,  loss: 5.1787e-03,  Y0: 3.0209e-01,  runtime:  138\n",
      "step:  1700,  loss: 3.9470e-03,  Y0: 3.0249e-01,  runtime:  142\n",
      "step:  1800,  loss: 2.9545e-03,  Y0: 3.0224e-01,  runtime:  146\n",
      "step:  1900,  loss: 2.3577e-03,  Y0: 3.0263e-01,  runtime:  150\n",
      "step:  2000,  loss: 1.8101e-03,  Y0: 3.0329e-01,  runtime:  154\n",
      "step:  2100,  loss: 1.3642e-03,  Y0: 3.0189e-01,  runtime:  158\n",
      "step:  2200,  loss: 9.5048e-04,  Y0: 3.0177e-01,  runtime:  162\n",
      "step:  2300,  loss: 7.2338e-04,  Y0: 3.0205e-01,  runtime:  166\n",
      "step:  2400,  loss: 5.0564e-04,  Y0: 3.0213e-01,  runtime:  170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  2500,  loss: 3.5618e-04,  Y0: 3.0210e-01,  runtime:  174\n",
      "step:  2600,  loss: 2.4691e-04,  Y0: 3.0174e-01,  runtime:  178\n",
      "step:  2700,  loss: 1.7131e-04,  Y0: 3.0228e-01,  runtime:  182\n",
      "step:  2800,  loss: 1.1665e-04,  Y0: 3.0215e-01,  runtime:  186\n",
      "step:  2900,  loss: 7.7697e-05,  Y0: 3.0206e-01,  runtime:  190\n",
      "step:  3000,  loss: 5.0304e-05,  Y0: 3.0217e-01,  runtime:  194\n",
      "step:  3100,  loss: 3.3039e-05,  Y0: 3.0199e-01,  runtime:  198\n",
      "step:  3200,  loss: 2.2586e-05,  Y0: 3.0206e-01,  runtime:  202\n",
      "step:  3300,  loss: 1.5292e-05,  Y0: 3.0192e-01,  runtime:  207\n",
      "step:  3400,  loss: 1.1653e-05,  Y0: 3.0192e-01,  runtime:  211\n",
      "step:  3500,  loss: 9.3553e-06,  Y0: 3.0207e-01,  runtime:  215\n",
      "step:  3600,  loss: 8.1584e-06,  Y0: 3.0207e-01,  runtime:  219\n",
      "step:  3700,  loss: 7.3772e-06,  Y0: 3.0205e-01,  runtime:  223\n",
      "step:  3800,  loss: 6.9677e-06,  Y0: 3.0193e-01,  runtime:  227\n",
      "step:  3900,  loss: 6.5576e-06,  Y0: 3.0205e-01,  runtime:  231\n",
      "step:  4000,  loss: 6.2241e-06,  Y0: 3.0205e-01,  runtime:  235\n",
      "running time: 235.790 s\n",
      "Begin to solve...\n",
      "step:     0,  loss: 5.8997e-02,  Y0: 4.5047e-01,  runtime:   52\n",
      "step:   100,  loss: 1.1377e-01,  Y0: 4.0682e-01,  runtime:   78\n",
      "step:   200,  loss: 1.0040e-01,  Y0: 3.6977e-01,  runtime:   82\n",
      "step:   300,  loss: 8.7403e-02,  Y0: 3.4364e-01,  runtime:   86\n",
      "step:   400,  loss: 7.2376e-02,  Y0: 3.2504e-01,  runtime:   90\n",
      "step:   500,  loss: 5.9019e-02,  Y0: 3.1327e-01,  runtime:   94\n",
      "step:   600,  loss: 5.0378e-02,  Y0: 3.0912e-01,  runtime:   97\n",
      "step:   700,  loss: 4.4675e-02,  Y0: 3.0634e-01,  runtime:  101\n",
      "step:   800,  loss: 3.7247e-02,  Y0: 3.0417e-01,  runtime:  105\n",
      "step:   900,  loss: 2.9978e-02,  Y0: 3.0246e-01,  runtime:  109\n",
      "step:  1000,  loss: 2.3829e-02,  Y0: 3.0114e-01,  runtime:  113\n",
      "step:  1100,  loss: 2.0888e-02,  Y0: 3.0074e-01,  runtime:  117\n",
      "step:  1200,  loss: 1.6335e-02,  Y0: 3.0094e-01,  runtime:  121\n",
      "step:  1300,  loss: 1.3397e-02,  Y0: 3.0269e-01,  runtime:  125\n",
      "step:  1400,  loss: 1.0052e-02,  Y0: 3.0068e-01,  runtime:  129\n",
      "step:  1500,  loss: 7.8054e-03,  Y0: 3.0142e-01,  runtime:  133\n",
      "step:  1600,  loss: 6.0657e-03,  Y0: 3.0171e-01,  runtime:  137\n",
      "step:  1700,  loss: 4.8753e-03,  Y0: 3.0225e-01,  runtime:  141\n",
      "step:  1800,  loss: 3.6856e-03,  Y0: 3.0145e-01,  runtime:  145\n",
      "step:  1900,  loss: 2.7606e-03,  Y0: 3.0235e-01,  runtime:  149\n",
      "step:  2000,  loss: 2.0495e-03,  Y0: 3.0161e-01,  runtime:  153\n",
      "step:  2100,  loss: 1.4625e-03,  Y0: 3.0180e-01,  runtime:  157\n",
      "step:  2200,  loss: 1.0524e-03,  Y0: 3.0222e-01,  runtime:  161\n",
      "step:  2300,  loss: 7.6895e-04,  Y0: 3.0189e-01,  runtime:  165\n",
      "step:  2400,  loss: 5.5049e-04,  Y0: 3.0254e-01,  runtime:  169\n",
      "step:  2500,  loss: 3.8799e-04,  Y0: 3.0214e-01,  runtime:  173\n",
      "step:  2600,  loss: 2.6147e-04,  Y0: 3.0204e-01,  runtime:  177\n",
      "step:  2700,  loss: 1.7063e-04,  Y0: 3.0202e-01,  runtime:  181\n",
      "step:  2800,  loss: 1.1418e-04,  Y0: 3.0214e-01,  runtime:  185\n",
      "step:  2900,  loss: 7.6956e-05,  Y0: 3.0218e-01,  runtime:  189\n",
      "step:  3000,  loss: 5.0413e-05,  Y0: 3.0207e-01,  runtime:  193\n",
      "step:  3100,  loss: 3.2661e-05,  Y0: 3.0197e-01,  runtime:  197\n",
      "step:  3200,  loss: 2.2220e-05,  Y0: 3.0177e-01,  runtime:  201\n",
      "step:  3300,  loss: 1.5308e-05,  Y0: 3.0206e-01,  runtime:  205\n",
      "step:  3400,  loss: 1.1516e-05,  Y0: 3.0212e-01,  runtime:  209\n",
      "step:  3500,  loss: 9.0019e-06,  Y0: 3.0206e-01,  runtime:  213\n",
      "step:  3600,  loss: 7.3605e-06,  Y0: 3.0208e-01,  runtime:  217\n",
      "step:  3700,  loss: 6.3974e-06,  Y0: 3.0204e-01,  runtime:  221\n",
      "step:  3800,  loss: 5.6760e-06,  Y0: 3.0204e-01,  runtime:  225\n",
      "step:  3900,  loss: 5.3657e-06,  Y0: 3.0200e-01,  runtime:  229\n",
      "step:  4000,  loss: 5.0553e-06,  Y0: 3.0200e-01,  runtime:  233\n",
      "running time: 233.219 s\n",
      "Begin to solve...\n",
      "step:     0,  loss: 5.9599e-02,  Y0: 4.5047e-01,  runtime:   52\n",
      "step:   100,  loss: 1.7399e-01,  Y0: 4.0516e-01,  runtime:   78\n",
      "step:   200,  loss: 1.5218e-01,  Y0: 3.7103e-01,  runtime:   82\n",
      "step:   300,  loss: 1.2669e-01,  Y0: 3.4123e-01,  runtime:   86\n",
      "step:   400,  loss: 1.4675e-01,  Y0: 3.2595e-01,  runtime:   91\n",
      "step:   500,  loss: 1.1136e-01,  Y0: 3.1543e-01,  runtime:   95\n",
      "step:   600,  loss: 9.1913e-02,  Y0: 3.0945e-01,  runtime:   98\n",
      "step:   700,  loss: 7.1425e-02,  Y0: 3.0662e-01,  runtime:  103\n",
      "step:   800,  loss: 6.0585e-02,  Y0: 3.0459e-01,  runtime:  107\n",
      "step:   900,  loss: 5.0252e-02,  Y0: 3.0288e-01,  runtime:  111\n",
      "step:  1000,  loss: 4.3577e-02,  Y0: 3.0419e-01,  runtime:  115\n",
      "step:  1100,  loss: 3.4723e-02,  Y0: 3.0146e-01,  runtime:  119\n",
      "step:  1200,  loss: 2.7593e-02,  Y0: 3.0182e-01,  runtime:  123\n",
      "step:  1300,  loss: 2.2108e-02,  Y0: 3.0203e-01,  runtime:  127\n",
      "step:  1400,  loss: 1.6755e-02,  Y0: 3.0105e-01,  runtime:  131\n",
      "step:  1500,  loss: 1.3343e-02,  Y0: 3.0315e-01,  runtime:  135\n",
      "step:  1600,  loss: 1.0055e-02,  Y0: 3.0305e-01,  runtime:  139\n",
      "step:  1700,  loss: 7.8773e-03,  Y0: 3.0315e-01,  runtime:  143\n",
      "step:  1800,  loss: 5.8849e-03,  Y0: 3.0179e-01,  runtime:  147\n",
      "step:  1900,  loss: 4.5990e-03,  Y0: 3.0034e-01,  runtime:  151\n",
      "step:  2000,  loss: 3.4044e-03,  Y0: 3.0156e-01,  runtime:  155\n",
      "step:  2100,  loss: 2.4562e-03,  Y0: 3.0148e-01,  runtime:  159\n",
      "step:  2200,  loss: 1.8066e-03,  Y0: 3.0241e-01,  runtime:  163\n",
      "step:  2300,  loss: 1.2426e-03,  Y0: 3.0228e-01,  runtime:  167\n",
      "step:  2400,  loss: 9.1538e-04,  Y0: 3.0224e-01,  runtime:  171\n",
      "step:  2500,  loss: 6.4676e-04,  Y0: 3.0211e-01,  runtime:  175\n",
      "step:  2600,  loss: 4.2351e-04,  Y0: 3.0210e-01,  runtime:  179\n",
      "step:  2700,  loss: 2.9243e-04,  Y0: 3.0202e-01,  runtime:  183\n",
      "step:  2800,  loss: 2.0715e-04,  Y0: 3.0220e-01,  runtime:  187\n",
      "step:  2900,  loss: 1.3552e-04,  Y0: 3.0191e-01,  runtime:  191\n",
      "step:  3000,  loss: 9.2368e-05,  Y0: 3.0188e-01,  runtime:  195\n",
      "step:  3100,  loss: 6.1116e-05,  Y0: 3.0210e-01,  runtime:  199\n",
      "step:  3200,  loss: 3.9964e-05,  Y0: 3.0210e-01,  runtime:  203\n",
      "step:  3300,  loss: 2.6849e-05,  Y0: 3.0199e-01,  runtime:  207\n",
      "step:  3400,  loss: 1.9893e-05,  Y0: 3.0205e-01,  runtime:  211\n",
      "step:  3500,  loss: 1.4632e-05,  Y0: 3.0201e-01,  runtime:  215\n",
      "step:  3600,  loss: 1.1600e-05,  Y0: 3.0203e-01,  runtime:  219\n",
      "step:  3700,  loss: 1.0233e-05,  Y0: 3.0207e-01,  runtime:  223\n",
      "step:  3800,  loss: 9.1513e-06,  Y0: 3.0211e-01,  runtime:  227\n",
      "step:  3900,  loss: 8.4106e-06,  Y0: 3.0205e-01,  runtime:  231\n",
      "step:  4000,  loss: 8.3013e-06,  Y0: 3.0207e-01,  runtime:  235\n",
      "running time: 235.181 s\n",
      "Begin to solve...\n",
      "step:     0,  loss: 6.8580e-02,  Y0: 4.5047e-01,  runtime:   52\n",
      "step:   100,  loss: 1.1141e-01,  Y0: 4.0579e-01,  runtime:   78\n",
      "step:   200,  loss: 9.4357e-02,  Y0: 3.6730e-01,  runtime:   82\n",
      "step:   300,  loss: 8.1580e-02,  Y0: 3.4289e-01,  runtime:   86\n",
      "step:   400,  loss: 6.6458e-02,  Y0: 3.2522e-01,  runtime:   90\n",
      "step:   500,  loss: 5.9141e-02,  Y0: 3.1690e-01,  runtime:   94\n",
      "step:   600,  loss: 4.9858e-02,  Y0: 3.0861e-01,  runtime:   98\n",
      "step:   700,  loss: 4.1871e-02,  Y0: 3.0529e-01,  runtime:  103\n",
      "step:   800,  loss: 3.4319e-02,  Y0: 3.0450e-01,  runtime:  107\n",
      "step:   900,  loss: 2.7792e-02,  Y0: 3.0175e-01,  runtime:  111\n",
      "step:  1000,  loss: 2.2678e-02,  Y0: 3.0170e-01,  runtime:  115\n",
      "step:  1100,  loss: 1.7910e-02,  Y0: 3.0345e-01,  runtime:  119\n",
      "step:  1200,  loss: 1.4490e-02,  Y0: 3.0386e-01,  runtime:  123\n",
      "step:  1300,  loss: 1.2082e-02,  Y0: 3.0257e-01,  runtime:  127\n",
      "step:  1400,  loss: 9.7624e-03,  Y0: 3.0334e-01,  runtime:  131\n",
      "step:  1500,  loss: 7.7425e-03,  Y0: 3.0192e-01,  runtime:  135\n",
      "step:  1600,  loss: 5.9187e-03,  Y0: 3.0070e-01,  runtime:  139\n",
      "step:  1700,  loss: 4.6014e-03,  Y0: 3.0219e-01,  runtime:  143\n",
      "step:  1800,  loss: 3.4817e-03,  Y0: 3.0283e-01,  runtime:  148\n",
      "step:  1900,  loss: 2.6790e-03,  Y0: 3.0196e-01,  runtime:  152\n",
      "step:  2000,  loss: 1.9864e-03,  Y0: 3.0217e-01,  runtime:  156\n",
      "step:  2100,  loss: 1.4351e-03,  Y0: 3.0155e-01,  runtime:  160\n",
      "step:  2200,  loss: 1.0447e-03,  Y0: 3.0144e-01,  runtime:  164\n",
      "step:  2300,  loss: 7.9058e-04,  Y0: 3.0273e-01,  runtime:  168\n",
      "step:  2400,  loss: 5.6329e-04,  Y0: 3.0248e-01,  runtime:  172\n",
      "step:  2500,  loss: 3.8341e-04,  Y0: 3.0192e-01,  runtime:  176\n",
      "step:  2600,  loss: 2.5960e-04,  Y0: 3.0191e-01,  runtime:  180\n",
      "step:  2700,  loss: 1.7945e-04,  Y0: 3.0165e-01,  runtime:  185\n",
      "step:  2800,  loss: 1.2124e-04,  Y0: 3.0212e-01,  runtime:  189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  2900,  loss: 8.5124e-05,  Y0: 3.0234e-01,  runtime:  193\n",
      "step:  3000,  loss: 5.6548e-05,  Y0: 3.0215e-01,  runtime:  197\n",
      "step:  3100,  loss: 3.7804e-05,  Y0: 3.0200e-01,  runtime:  201\n",
      "step:  3200,  loss: 2.5441e-05,  Y0: 3.0188e-01,  runtime:  205\n",
      "step:  3300,  loss: 1.9537e-05,  Y0: 3.0204e-01,  runtime:  209\n",
      "step:  3400,  loss: 1.4420e-05,  Y0: 3.0199e-01,  runtime:  214\n",
      "step:  3500,  loss: 1.1519e-05,  Y0: 3.0196e-01,  runtime:  218\n",
      "step:  3600,  loss: 9.8291e-06,  Y0: 3.0206e-01,  runtime:  222\n",
      "step:  3700,  loss: 8.6218e-06,  Y0: 3.0204e-01,  runtime:  226\n",
      "step:  3800,  loss: 7.9718e-06,  Y0: 3.0202e-01,  runtime:  229\n",
      "step:  3900,  loss: 7.7140e-06,  Y0: 3.0208e-01,  runtime:  233\n",
      "step:  4000,  loss: 7.5111e-06,  Y0: 3.0202e-01,  runtime:  238\n",
      "running time: 238.049 s\n",
      "Begin to solve...\n",
      "step:     0,  loss: 5.8365e-02,  Y0: 4.5047e-01,  runtime:   52\n",
      "step:   100,  loss: 1.0896e-01,  Y0: 4.0753e-01,  runtime:   79\n",
      "step:   200,  loss: 9.6246e-02,  Y0: 3.7137e-01,  runtime:   83\n",
      "step:   300,  loss: 8.2383e-02,  Y0: 3.4275e-01,  runtime:   87\n",
      "step:   400,  loss: 6.7014e-02,  Y0: 3.2587e-01,  runtime:   90\n",
      "step:   500,  loss: 5.5887e-02,  Y0: 3.1582e-01,  runtime:   94\n",
      "step:   600,  loss: 5.0980e-02,  Y0: 3.0986e-01,  runtime:   98\n",
      "step:   700,  loss: 4.1248e-02,  Y0: 3.0568e-01,  runtime:  102\n",
      "step:   800,  loss: 3.2884e-02,  Y0: 3.0224e-01,  runtime:  107\n",
      "step:   900,  loss: 2.6946e-02,  Y0: 3.0465e-01,  runtime:  111\n",
      "step:  1000,  loss: 2.1665e-02,  Y0: 3.0268e-01,  runtime:  115\n",
      "step:  1100,  loss: 1.7894e-02,  Y0: 3.0237e-01,  runtime:  119\n",
      "step:  1200,  loss: 1.4682e-02,  Y0: 3.0431e-01,  runtime:  123\n",
      "step:  1300,  loss: 1.1600e-02,  Y0: 3.0275e-01,  runtime:  127\n",
      "step:  1400,  loss: 9.3776e-03,  Y0: 3.0231e-01,  runtime:  130\n",
      "step:  1500,  loss: 7.3860e-03,  Y0: 3.0249e-01,  runtime:  134\n",
      "step:  1600,  loss: 5.7127e-03,  Y0: 3.0228e-01,  runtime:  138\n",
      "step:  1700,  loss: 4.4337e-03,  Y0: 3.0202e-01,  runtime:  142\n",
      "step:  1800,  loss: 3.2408e-03,  Y0: 3.0117e-01,  runtime:  147\n",
      "step:  1900,  loss: 2.4881e-03,  Y0: 3.0251e-01,  runtime:  150\n",
      "step:  2000,  loss: 1.8460e-03,  Y0: 3.0281e-01,  runtime:  154\n",
      "step:  2100,  loss: 1.3211e-03,  Y0: 3.0199e-01,  runtime:  158\n",
      "step:  2200,  loss: 9.4996e-04,  Y0: 3.0227e-01,  runtime:  162\n",
      "step:  2300,  loss: 6.9708e-04,  Y0: 3.0213e-01,  runtime:  166\n",
      "step:  2400,  loss: 4.7981e-04,  Y0: 3.0182e-01,  runtime:  170\n",
      "step:  2500,  loss: 3.3452e-04,  Y0: 3.0191e-01,  runtime:  174\n",
      "step:  2600,  loss: 2.3143e-04,  Y0: 3.0235e-01,  runtime:  178\n",
      "step:  2700,  loss: 1.4981e-04,  Y0: 3.0204e-01,  runtime:  182\n",
      "step:  2800,  loss: 1.0067e-04,  Y0: 3.0194e-01,  runtime:  186\n",
      "step:  2900,  loss: 6.5123e-05,  Y0: 3.0224e-01,  runtime:  190\n",
      "step:  3000,  loss: 4.2893e-05,  Y0: 3.0204e-01,  runtime:  194\n",
      "step:  3100,  loss: 2.8030e-05,  Y0: 3.0202e-01,  runtime:  198\n",
      "step:  3200,  loss: 1.8527e-05,  Y0: 3.0206e-01,  runtime:  202\n",
      "step:  3300,  loss: 1.3553e-05,  Y0: 3.0213e-01,  runtime:  206\n",
      "step:  3400,  loss: 9.9999e-06,  Y0: 3.0211e-01,  runtime:  210\n",
      "step:  3500,  loss: 8.0190e-06,  Y0: 3.0207e-01,  runtime:  214\n",
      "step:  3600,  loss: 6.4422e-06,  Y0: 3.0206e-01,  runtime:  218\n",
      "step:  3700,  loss: 5.7213e-06,  Y0: 3.0201e-01,  runtime:  222\n",
      "step:  3800,  loss: 5.3139e-06,  Y0: 3.0204e-01,  runtime:  225\n",
      "step:  3900,  loss: 5.1004e-06,  Y0: 3.0203e-01,  runtime:  229\n",
      "step:  4000,  loss: 4.8735e-06,  Y0: 3.0208e-01,  runtime:  233\n",
      "running time: 233.928 s\n",
      "Begin to solve...\n",
      "step:     0,  loss: 6.1435e-02,  Y0: 4.5047e-01,  runtime:   52\n",
      "step:   100,  loss: 1.3326e-01,  Y0: 4.0617e-01,  runtime:   78\n",
      "step:   200,  loss: 1.1405e-01,  Y0: 3.7053e-01,  runtime:   82\n",
      "step:   300,  loss: 9.1817e-02,  Y0: 3.3945e-01,  runtime:   86\n",
      "step:   400,  loss: 7.4845e-02,  Y0: 3.2303e-01,  runtime:   90\n",
      "step:   500,  loss: 6.2555e-02,  Y0: 3.1290e-01,  runtime:   94\n",
      "step:   600,  loss: 5.2750e-02,  Y0: 3.0791e-01,  runtime:   98\n",
      "step:   700,  loss: 4.4118e-02,  Y0: 3.0296e-01,  runtime:  102\n",
      "step:   800,  loss: 3.8071e-02,  Y0: 3.0383e-01,  runtime:  106\n",
      "step:   900,  loss: 3.1934e-02,  Y0: 3.0074e-01,  runtime:  110\n",
      "step:  1000,  loss: 2.5349e-02,  Y0: 3.0010e-01,  runtime:  114\n",
      "step:  1100,  loss: 1.9496e-02,  Y0: 3.0309e-01,  runtime:  118\n",
      "step:  1200,  loss: 1.5820e-02,  Y0: 3.0255e-01,  runtime:  121\n",
      "step:  1300,  loss: 1.2523e-02,  Y0: 3.0455e-01,  runtime:  125\n",
      "step:  1400,  loss: 9.7173e-03,  Y0: 3.0165e-01,  runtime:  129\n",
      "step:  1500,  loss: 7.7090e-03,  Y0: 3.0211e-01,  runtime:  133\n",
      "step:  1600,  loss: 5.8938e-03,  Y0: 3.0331e-01,  runtime:  137\n",
      "step:  1700,  loss: 4.6468e-03,  Y0: 3.0196e-01,  runtime:  141\n",
      "step:  1800,  loss: 3.4551e-03,  Y0: 3.0141e-01,  runtime:  144\n",
      "step:  1900,  loss: 2.5966e-03,  Y0: 3.0196e-01,  runtime:  148\n",
      "step:  2000,  loss: 1.8841e-03,  Y0: 3.0186e-01,  runtime:  152\n",
      "step:  2100,  loss: 1.3569e-03,  Y0: 3.0278e-01,  runtime:  156\n",
      "step:  2200,  loss: 1.0134e-03,  Y0: 3.0203e-01,  runtime:  160\n",
      "step:  2300,  loss: 6.8711e-04,  Y0: 3.0226e-01,  runtime:  164\n",
      "step:  2400,  loss: 4.8892e-04,  Y0: 3.0203e-01,  runtime:  168\n",
      "step:  2500,  loss: 3.5187e-04,  Y0: 3.0212e-01,  runtime:  172\n",
      "step:  2600,  loss: 2.3360e-04,  Y0: 3.0199e-01,  runtime:  176\n",
      "step:  2700,  loss: 1.5501e-04,  Y0: 3.0210e-01,  runtime:  180\n",
      "step:  2800,  loss: 1.0318e-04,  Y0: 3.0219e-01,  runtime:  184\n",
      "step:  2900,  loss: 6.5657e-05,  Y0: 3.0198e-01,  runtime:  188\n",
      "step:  3000,  loss: 4.2719e-05,  Y0: 3.0208e-01,  runtime:  192\n",
      "step:  3100,  loss: 2.8747e-05,  Y0: 3.0220e-01,  runtime:  195\n",
      "step:  3200,  loss: 1.9669e-05,  Y0: 3.0215e-01,  runtime:  199\n",
      "step:  3300,  loss: 1.4389e-05,  Y0: 3.0209e-01,  runtime:  203\n",
      "step:  3400,  loss: 1.1143e-05,  Y0: 3.0213e-01,  runtime:  207\n",
      "step:  3500,  loss: 9.2298e-06,  Y0: 3.0198e-01,  runtime:  211\n",
      "step:  3600,  loss: 7.6553e-06,  Y0: 3.0199e-01,  runtime:  214\n",
      "step:  3700,  loss: 7.4573e-06,  Y0: 3.0206e-01,  runtime:  218\n",
      "step:  3800,  loss: 7.4641e-06,  Y0: 3.0196e-01,  runtime:  222\n",
      "step:  3900,  loss: 6.9270e-06,  Y0: 3.0200e-01,  runtime:  226\n",
      "step:  4000,  loss: 6.5797e-06,  Y0: 3.0199e-01,  runtime:  230\n",
      "running time: 230.634 s\n",
      "Begin to solve...\n",
      "step:     0,  loss: 5.0042e-02,  Y0: 4.5047e-01,  runtime:   52\n",
      "step:   100,  loss: 1.0183e-01,  Y0: 4.0572e-01,  runtime:   78\n",
      "step:   200,  loss: 9.3344e-02,  Y0: 3.7037e-01,  runtime:   82\n",
      "step:   300,  loss: 8.2483e-02,  Y0: 3.4338e-01,  runtime:   86\n",
      "step:   400,  loss: 7.3040e-02,  Y0: 3.2663e-01,  runtime:   90\n",
      "step:   500,  loss: 6.1805e-02,  Y0: 3.1485e-01,  runtime:   94\n",
      "step:   600,  loss: 5.0965e-02,  Y0: 3.0785e-01,  runtime:   98\n",
      "step:   700,  loss: 4.1966e-02,  Y0: 3.0348e-01,  runtime:  101\n",
      "step:   800,  loss: 3.5567e-02,  Y0: 3.0561e-01,  runtime:  105\n",
      "step:   900,  loss: 2.9329e-02,  Y0: 3.0249e-01,  runtime:  109\n",
      "step:  1000,  loss: 2.5012e-02,  Y0: 3.0236e-01,  runtime:  113\n",
      "step:  1100,  loss: 2.0278e-02,  Y0: 3.0101e-01,  runtime:  117\n",
      "step:  1200,  loss: 1.6095e-02,  Y0: 3.0211e-01,  runtime:  121\n",
      "step:  1300,  loss: 1.3586e-02,  Y0: 3.0173e-01,  runtime:  125\n",
      "step:  1400,  loss: 1.0805e-02,  Y0: 3.0253e-01,  runtime:  129\n",
      "step:  1500,  loss: 8.4654e-03,  Y0: 3.0137e-01,  runtime:  133\n",
      "step:  1600,  loss: 6.3592e-03,  Y0: 3.0153e-01,  runtime:  136\n",
      "step:  1700,  loss: 4.8307e-03,  Y0: 3.0112e-01,  runtime:  140\n",
      "step:  1800,  loss: 3.7847e-03,  Y0: 3.0300e-01,  runtime:  144\n",
      "step:  1900,  loss: 2.9197e-03,  Y0: 3.0286e-01,  runtime:  148\n",
      "step:  2000,  loss: 2.2219e-03,  Y0: 3.0209e-01,  runtime:  152\n",
      "step:  2100,  loss: 1.6233e-03,  Y0: 3.0264e-01,  runtime:  156\n",
      "step:  2200,  loss: 1.1501e-03,  Y0: 3.0164e-01,  runtime:  160\n",
      "step:  2300,  loss: 8.6209e-04,  Y0: 3.0186e-01,  runtime:  164\n",
      "step:  2400,  loss: 6.1857e-04,  Y0: 3.0242e-01,  runtime:  167\n",
      "step:  2500,  loss: 4.2598e-04,  Y0: 3.0273e-01,  runtime:  171\n",
      "step:  2600,  loss: 2.8297e-04,  Y0: 3.0198e-01,  runtime:  175\n",
      "step:  2700,  loss: 1.9370e-04,  Y0: 3.0177e-01,  runtime:  179\n",
      "step:  2800,  loss: 1.3251e-04,  Y0: 3.0222e-01,  runtime:  183\n",
      "step:  2900,  loss: 8.5414e-05,  Y0: 3.0209e-01,  runtime:  187\n",
      "step:  3000,  loss: 5.7159e-05,  Y0: 3.0191e-01,  runtime:  190\n",
      "step:  3100,  loss: 3.7015e-05,  Y0: 3.0200e-01,  runtime:  195\n",
      "step:  3200,  loss: 2.4090e-05,  Y0: 3.0207e-01,  runtime:  199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  3300,  loss: 1.5642e-05,  Y0: 3.0193e-01,  runtime:  202\n",
      "step:  3400,  loss: 1.0896e-05,  Y0: 3.0196e-01,  runtime:  206\n",
      "step:  3500,  loss: 8.2952e-06,  Y0: 3.0205e-01,  runtime:  210\n",
      "step:  3600,  loss: 6.6655e-06,  Y0: 3.0209e-01,  runtime:  214\n",
      "step:  3700,  loss: 5.8174e-06,  Y0: 3.0204e-01,  runtime:  218\n",
      "step:  3800,  loss: 5.4866e-06,  Y0: 3.0202e-01,  runtime:  222\n",
      "step:  3900,  loss: 5.4644e-06,  Y0: 3.0206e-01,  runtime:  226\n",
      "step:  4000,  loss: 5.4149e-06,  Y0: 3.0207e-01,  runtime:  229\n",
      "running time: 229.793 s\n",
      "Begin to solve...\n",
      "step:     0,  loss: 5.7374e-02,  Y0: 4.5047e-01,  runtime:   52\n",
      "step:   100,  loss: 1.0787e-01,  Y0: 4.0617e-01,  runtime:   78\n",
      "step:   200,  loss: 9.7172e-02,  Y0: 3.7154e-01,  runtime:   82\n",
      "step:   300,  loss: 8.0649e-02,  Y0: 3.4520e-01,  runtime:   86\n",
      "step:   400,  loss: 6.5278e-02,  Y0: 3.2490e-01,  runtime:   90\n",
      "step:   500,  loss: 5.2737e-02,  Y0: 3.1275e-01,  runtime:   94\n",
      "step:   600,  loss: 4.6713e-02,  Y0: 3.0695e-01,  runtime:   98\n",
      "step:   700,  loss: 3.6908e-02,  Y0: 3.0253e-01,  runtime:  102\n",
      "step:   800,  loss: 3.0792e-02,  Y0: 3.0226e-01,  runtime:  106\n",
      "step:   900,  loss: 2.4249e-02,  Y0: 3.0127e-01,  runtime:  110\n",
      "step:  1000,  loss: 1.9534e-02,  Y0: 3.0058e-01,  runtime:  114\n",
      "step:  1100,  loss: 1.6600e-02,  Y0: 2.9913e-01,  runtime:  118\n",
      "step:  1200,  loss: 1.3111e-02,  Y0: 2.9973e-01,  runtime:  121\n",
      "step:  1300,  loss: 1.0530e-02,  Y0: 3.0096e-01,  runtime:  125\n",
      "step:  1400,  loss: 8.1216e-03,  Y0: 3.0207e-01,  runtime:  129\n",
      "step:  1500,  loss: 6.3230e-03,  Y0: 3.0230e-01,  runtime:  133\n",
      "step:  1600,  loss: 4.9266e-03,  Y0: 3.0037e-01,  runtime:  137\n",
      "step:  1700,  loss: 3.7128e-03,  Y0: 3.0182e-01,  runtime:  141\n",
      "step:  1800,  loss: 2.7327e-03,  Y0: 3.0231e-01,  runtime:  145\n",
      "step:  1900,  loss: 2.0240e-03,  Y0: 3.0253e-01,  runtime:  149\n",
      "step:  2000,  loss: 1.4976e-03,  Y0: 3.0226e-01,  runtime:  153\n",
      "step:  2100,  loss: 1.0884e-03,  Y0: 3.0116e-01,  runtime:  157\n",
      "step:  2200,  loss: 8.0358e-04,  Y0: 3.0268e-01,  runtime:  161\n",
      "step:  2300,  loss: 5.4879e-04,  Y0: 3.0218e-01,  runtime:  165\n",
      "step:  2400,  loss: 3.8210e-04,  Y0: 3.0205e-01,  runtime:  169\n",
      "step:  2500,  loss: 2.5215e-04,  Y0: 3.0195e-01,  runtime:  172\n",
      "step:  2600,  loss: 1.6650e-04,  Y0: 3.0218e-01,  runtime:  176\n",
      "step:  2700,  loss: 1.1379e-04,  Y0: 3.0207e-01,  runtime:  180\n",
      "step:  2800,  loss: 7.5188e-05,  Y0: 3.0209e-01,  runtime:  184\n",
      "step:  2900,  loss: 5.3518e-05,  Y0: 3.0204e-01,  runtime:  188\n",
      "step:  3000,  loss: 3.6326e-05,  Y0: 3.0209e-01,  runtime:  192\n",
      "step:  3100,  loss: 2.6171e-05,  Y0: 3.0210e-01,  runtime:  196\n",
      "step:  3200,  loss: 1.8286e-05,  Y0: 3.0207e-01,  runtime:  200\n",
      "step:  3300,  loss: 1.4273e-05,  Y0: 3.0208e-01,  runtime:  204\n",
      "step:  3400,  loss: 1.1474e-05,  Y0: 3.0199e-01,  runtime:  208\n",
      "step:  3500,  loss: 9.3488e-06,  Y0: 3.0201e-01,  runtime:  212\n",
      "step:  3600,  loss: 7.6071e-06,  Y0: 3.0189e-01,  runtime:  216\n",
      "step:  3700,  loss: 7.1072e-06,  Y0: 3.0208e-01,  runtime:  220\n",
      "step:  3800,  loss: 7.0134e-06,  Y0: 3.0205e-01,  runtime:  223\n",
      "step:  3900,  loss: 6.8791e-06,  Y0: 3.0190e-01,  runtime:  227\n",
      "step:  4000,  loss: 6.6034e-06,  Y0: 3.0208e-01,  runtime:  231\n",
      "running time: 231.832 s\n",
      "Begin to solve...\n",
      "step:     0,  loss: 5.3019e-02,  Y0: 4.5047e-01,  runtime:   52\n",
      "step:   100,  loss: 1.0104e-01,  Y0: 4.0530e-01,  runtime:   78\n",
      "step:   200,  loss: 8.9214e-02,  Y0: 3.6934e-01,  runtime:   82\n",
      "step:   300,  loss: 7.6265e-02,  Y0: 3.4448e-01,  runtime:   86\n",
      "step:   400,  loss: 6.4412e-02,  Y0: 3.2747e-01,  runtime:   90\n",
      "step:   500,  loss: 5.3610e-02,  Y0: 3.1571e-01,  runtime:   93\n",
      "step:   600,  loss: 4.3478e-02,  Y0: 3.1246e-01,  runtime:   97\n",
      "step:   700,  loss: 3.5360e-02,  Y0: 3.0619e-01,  runtime:  101\n",
      "step:   800,  loss: 2.9688e-02,  Y0: 3.0419e-01,  runtime:  105\n",
      "step:   900,  loss: 2.3533e-02,  Y0: 3.0205e-01,  runtime:  109\n",
      "step:  1000,  loss: 2.0220e-02,  Y0: 3.0164e-01,  runtime:  113\n",
      "step:  1100,  loss: 1.5992e-02,  Y0: 3.0072e-01,  runtime:  116\n",
      "step:  1200,  loss: 1.2909e-02,  Y0: 3.0294e-01,  runtime:  120\n",
      "step:  1300,  loss: 1.0634e-02,  Y0: 3.0273e-01,  runtime:  124\n",
      "step:  1400,  loss: 8.2809e-03,  Y0: 3.0024e-01,  runtime:  128\n",
      "step:  1500,  loss: 6.5480e-03,  Y0: 3.0247e-01,  runtime:  132\n",
      "step:  1600,  loss: 4.9513e-03,  Y0: 3.0234e-01,  runtime:  135\n",
      "step:  1700,  loss: 3.8497e-03,  Y0: 3.0045e-01,  runtime:  139\n",
      "step:  1800,  loss: 3.0587e-03,  Y0: 3.0125e-01,  runtime:  143\n",
      "step:  1900,  loss: 2.2733e-03,  Y0: 3.0217e-01,  runtime:  147\n",
      "step:  2000,  loss: 1.7672e-03,  Y0: 3.0206e-01,  runtime:  151\n",
      "step:  2100,  loss: 1.3224e-03,  Y0: 3.0231e-01,  runtime:  154\n",
      "step:  2200,  loss: 8.8381e-04,  Y0: 3.0157e-01,  runtime:  158\n",
      "step:  2300,  loss: 6.6057e-04,  Y0: 3.0161e-01,  runtime:  162\n",
      "step:  2400,  loss: 4.9598e-04,  Y0: 3.0246e-01,  runtime:  166\n",
      "step:  2500,  loss: 3.2792e-04,  Y0: 3.0224e-01,  runtime:  169\n",
      "step:  2600,  loss: 2.4047e-04,  Y0: 3.0191e-01,  runtime:  173\n",
      "step:  2700,  loss: 1.5718e-04,  Y0: 3.0165e-01,  runtime:  177\n",
      "step:  2800,  loss: 1.0745e-04,  Y0: 3.0176e-01,  runtime:  181\n",
      "step:  2900,  loss: 7.3547e-05,  Y0: 3.0185e-01,  runtime:  185\n",
      "step:  3000,  loss: 4.8738e-05,  Y0: 3.0194e-01,  runtime:  188\n",
      "step:  3100,  loss: 3.0546e-05,  Y0: 3.0210e-01,  runtime:  192\n",
      "step:  3200,  loss: 2.1164e-05,  Y0: 3.0212e-01,  runtime:  196\n",
      "step:  3300,  loss: 1.4170e-05,  Y0: 3.0202e-01,  runtime:  200\n",
      "step:  3400,  loss: 1.0448e-05,  Y0: 3.0200e-01,  runtime:  203\n",
      "step:  3500,  loss: 8.1877e-06,  Y0: 3.0204e-01,  runtime:  207\n",
      "step:  3600,  loss: 6.7684e-06,  Y0: 3.0197e-01,  runtime:  211\n",
      "step:  3700,  loss: 5.7002e-06,  Y0: 3.0208e-01,  runtime:  215\n",
      "step:  3800,  loss: 5.3188e-06,  Y0: 3.0206e-01,  runtime:  218\n",
      "step:  3900,  loss: 4.9575e-06,  Y0: 3.0202e-01,  runtime:  222\n",
      "step:  4000,  loss: 4.8908e-06,  Y0: 3.0206e-01,  runtime:  226\n",
      "running time: 226.692 s\n",
      "Begin to solve...\n",
      "step:     0,  loss: 5.6240e-02,  Y0: 4.5047e-01,  runtime:   52\n",
      "step:   100,  loss: 1.0562e-01,  Y0: 4.0577e-01,  runtime:   78\n",
      "step:   200,  loss: 8.7298e-02,  Y0: 3.6874e-01,  runtime:   82\n",
      "step:   300,  loss: 6.9147e-02,  Y0: 3.4153e-01,  runtime:   86\n",
      "step:   400,  loss: 6.0781e-02,  Y0: 3.2150e-01,  runtime:   90\n",
      "step:   500,  loss: 5.2631e-02,  Y0: 3.1192e-01,  runtime:   94\n",
      "step:   600,  loss: 4.2240e-02,  Y0: 3.0592e-01,  runtime:   97\n",
      "step:   700,  loss: 3.5802e-02,  Y0: 3.0286e-01,  runtime:  101\n",
      "step:   800,  loss: 2.9347e-02,  Y0: 3.0319e-01,  runtime:  105\n",
      "step:   900,  loss: 2.4762e-02,  Y0: 3.0196e-01,  runtime:  109\n",
      "step:  1000,  loss: 2.0253e-02,  Y0: 3.0246e-01,  runtime:  113\n",
      "step:  1100,  loss: 1.6628e-02,  Y0: 3.0150e-01,  runtime:  117\n",
      "step:  1200,  loss: 1.3469e-02,  Y0: 3.0292e-01,  runtime:  121\n",
      "step:  1300,  loss: 1.0977e-02,  Y0: 3.0194e-01,  runtime:  125\n",
      "step:  1400,  loss: 8.4072e-03,  Y0: 3.0152e-01,  runtime:  128\n",
      "step:  1500,  loss: 6.8135e-03,  Y0: 3.0289e-01,  runtime:  132\n",
      "step:  1600,  loss: 5.1852e-03,  Y0: 3.0178e-01,  runtime:  136\n",
      "step:  1700,  loss: 3.9379e-03,  Y0: 3.0220e-01,  runtime:  140\n",
      "step:  1800,  loss: 2.9024e-03,  Y0: 3.0260e-01,  runtime:  144\n",
      "step:  1900,  loss: 2.2898e-03,  Y0: 3.0236e-01,  runtime:  148\n",
      "step:  2000,  loss: 1.7129e-03,  Y0: 3.0209e-01,  runtime:  152\n",
      "step:  2100,  loss: 1.2698e-03,  Y0: 3.0202e-01,  runtime:  156\n",
      "step:  2200,  loss: 8.9551e-04,  Y0: 3.0259e-01,  runtime:  160\n",
      "step:  2300,  loss: 6.3298e-04,  Y0: 3.0188e-01,  runtime:  164\n",
      "step:  2400,  loss: 4.3461e-04,  Y0: 3.0204e-01,  runtime:  168\n",
      "step:  2500,  loss: 3.1267e-04,  Y0: 3.0216e-01,  runtime:  172\n",
      "step:  2600,  loss: 2.1722e-04,  Y0: 3.0215e-01,  runtime:  176\n",
      "step:  2700,  loss: 1.5351e-04,  Y0: 3.0190e-01,  runtime:  179\n",
      "step:  2800,  loss: 9.8053e-05,  Y0: 3.0198e-01,  runtime:  183\n",
      "step:  2900,  loss: 6.2570e-05,  Y0: 3.0211e-01,  runtime:  187\n",
      "step:  3000,  loss: 4.0212e-05,  Y0: 3.0210e-01,  runtime:  191\n",
      "step:  3100,  loss: 2.5212e-05,  Y0: 3.0201e-01,  runtime:  195\n",
      "step:  3200,  loss: 1.7213e-05,  Y0: 3.0199e-01,  runtime:  199\n",
      "step:  3300,  loss: 1.2075e-05,  Y0: 3.0194e-01,  runtime:  203\n",
      "step:  3400,  loss: 8.8575e-06,  Y0: 3.0192e-01,  runtime:  206\n",
      "step:  3500,  loss: 6.9508e-06,  Y0: 3.0200e-01,  runtime:  210\n",
      "step:  3600,  loss: 5.9827e-06,  Y0: 3.0209e-01,  runtime:  214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  3700,  loss: 5.3885e-06,  Y0: 3.0196e-01,  runtime:  218\n",
      "step:  3800,  loss: 5.0503e-06,  Y0: 3.0196e-01,  runtime:  222\n",
      "step:  3900,  loss: 5.1498e-06,  Y0: 3.0206e-01,  runtime:  226\n",
      "step:  4000,  loss: 5.2647e-06,  Y0: 3.0204e-01,  runtime:  230\n",
      "running time: 230.509 s\n"
     ]
    }
   ],
   "source": [
    "!python main.py\n",
    "!python regression.py\n",
    "!python deep_split.py\n",
    "!python deep_signature/ppde_asian.py\n",
    "# !python deep_signature/ppde_barrier.py\n",
    "!python deep_galerkin.py\n",
    "!python deep_bsde.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1613,
     "status": "ok",
     "timestamp": 1656559463614,
     "user": {
      "displayName": "nguwi jy",
      "userId": "17294038731682000497"
     },
     "user_tz": -480
    },
    "id": "XackWqM8k2nm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   problem_name                                        scheme_name deep  dim  \\\n17  Asian, T=.1   Deep PPDE using \\eqref{eq:deep_scheme_nonlinear}    Y    1   \n20  Asian, T=.1  Deep PPDE using \\eqref{eq:var_reduced_deep_sch...    Y    1   \n23  Asian, T=.1                                Deep PPDE with LSTM    Y    1   \n26  Asian, T=.1  \\cite{ren2017convergence} using \\eqref{eq:prob...    N    1   \n29  Asian, T=.1  \\cite{ren2017convergence} using \\eqref{eq:var_...    N    1   \n32  Asian, T=.1                            \\cite{saporito2020pdgm}    Y    1   \n34  Asian, T=.1                           \\cite{sabate2020solving}    Y    1   \n36  Asian, T=.1                              \\cite{han2018solving}    Y    1   \n37  Asian, T=.1                                \\cite{beck2019deep}    Y    1   \n\n        mean       std  exact  l1_error  runtime  \n17  0.300816  0.001287      1  0.003826       31  \n20  0.300816  0.001287      1  0.003826       31  \n23  0.299338  0.006787      1  0.012878       46  \n26  0.300277  0.000223      1  0.000477        1  \n29  0.300277  0.000223      1  0.000477        1  \n32  0.300254  0.002433      1  0.006007       25  \n34  0.300272  0.001236      1  0.003514       10  \n36  0.300247  0.000002      1  0.000149       32  \n37  0.300283  0.000421      1  0.001122       20  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>problem_name</th>\n      <th>scheme_name</th>\n      <th>deep</th>\n      <th>dim</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>exact</th>\n      <th>l1_error</th>\n      <th>runtime</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17</th>\n      <td>Asian, T=.1</td>\n      <td>Deep PPDE using \\eqref{eq:deep_scheme_nonlinear}</td>\n      <td>Y</td>\n      <td>1</td>\n      <td>0.300816</td>\n      <td>0.001287</td>\n      <td>1</td>\n      <td>0.003826</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Asian, T=.1</td>\n      <td>Deep PPDE using \\eqref{eq:var_reduced_deep_sch...</td>\n      <td>Y</td>\n      <td>1</td>\n      <td>0.300816</td>\n      <td>0.001287</td>\n      <td>1</td>\n      <td>0.003826</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Asian, T=.1</td>\n      <td>Deep PPDE with LSTM</td>\n      <td>Y</td>\n      <td>1</td>\n      <td>0.299338</td>\n      <td>0.006787</td>\n      <td>1</td>\n      <td>0.012878</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>Asian, T=.1</td>\n      <td>\\cite{ren2017convergence} using \\eqref{eq:prob...</td>\n      <td>N</td>\n      <td>1</td>\n      <td>0.300277</td>\n      <td>0.000223</td>\n      <td>1</td>\n      <td>0.000477</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>Asian, T=.1</td>\n      <td>\\cite{ren2017convergence} using \\eqref{eq:var_...</td>\n      <td>N</td>\n      <td>1</td>\n      <td>0.300277</td>\n      <td>0.000223</td>\n      <td>1</td>\n      <td>0.000477</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>Asian, T=.1</td>\n      <td>\\cite{saporito2020pdgm}</td>\n      <td>Y</td>\n      <td>1</td>\n      <td>0.300254</td>\n      <td>0.002433</td>\n      <td>1</td>\n      <td>0.006007</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>Asian, T=.1</td>\n      <td>\\cite{sabate2020solving}</td>\n      <td>Y</td>\n      <td>1</td>\n      <td>0.300272</td>\n      <td>0.001236</td>\n      <td>1</td>\n      <td>0.003514</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>Asian, T=.1</td>\n      <td>\\cite{han2018solving}</td>\n      <td>Y</td>\n      <td>1</td>\n      <td>0.300247</td>\n      <td>0.000002</td>\n      <td>1</td>\n      <td>0.000149</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>Asian, T=.1</td>\n      <td>\\cite{beck2019deep}</td>\n      <td>Y</td>\n      <td>1</td>\n      <td>0.300283</td>\n      <td>0.000421</td>\n      <td>1</td>\n      <td>0.001122</td>\n      <td>20</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarize the statistics\n",
    "# !python stats.py\n",
    "from stats import res\n",
    "res[(res.problem_name == 'Asian, T=.1') & (res.dim == 1)]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNBms+xEAVuyLGzA47b2Or9",
   "collapsed_sections": [],
   "mount_file_id": "1odNb4h3Lk1s80b_3VL0cchrNZ7876zSO",
   "name": "comparison.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
